{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech-Emotion-Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbPi1c9YGmtJHVzzez2icJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vidhuran-Rajendran/Speech-Emotion-Recognition/blob/main/Speech_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPEECH EMOTION RECOGNITION**"
      ],
      "metadata": {
        "id": "qiWAT0yavP6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TABLE OF CONTENTS\n",
        "\n",
        "* INTRODUCTION\n",
        "\n",
        "* EXPLORATORY DATA ANALYSIS (EDA)\n",
        "\n",
        "* DATA AUGMENTATION\n",
        "\n",
        "* FEATURE EXTRACTION\n",
        "\n",
        "* MODEL"
      ],
      "metadata": {
        "id": "3GCT3EABvZOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INTRODUCTION**\n",
        "\n",
        "## **PROBLEM STATEMENT**\n",
        "\n",
        "The purpose is to recognize the emotion and affective state of the speaker from his/her speech signal.\n",
        "\n",
        "\n",
        "### **DATA SOURCE USED**\n",
        "\n",
        "We have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. Here's the filename identifiers as per the official RAVDESS website:\n",
        "\n",
        "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "* Vocal channel (01 = speech, 02 = song).\n",
        "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "Here's an example of an audio filename. 02-01-06-01-02-01-12.mp4"
      ],
      "metadata": {
        "id": "_1ugJ8QNwT73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXPLORATORY DATA ANALYSIS**\n",
        "The key features of the audio data are namely, MFCC (Mel Frequency Cepstral Coefficients) and Mel Spectrogram\n",
        "\n",
        "* MFCC (Mel Frequency Cepstral Coefficients)- MFCC is important feature extraction when using speech data,Mel scale is a scale that relates the perceived frequency of a tone to the real measured frequency. It scales the frequency so that you can fit greater carefully what the human ear can hear\n",
        "* Mel Spectrogram- A Fast Fourier Transform is computed on overlapping windowed segments of the signal, and what is the spectrogram , Spectrogram is visual way of representation of signal strength and also use for display the frequency sound waves.\n",
        "For the EDA we have used MFCC and Mel Spectogram"
      ],
      "metadata": {
        "id": "dvUpSmRWwqqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT THE LIBRARIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to play the audio files\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM,BatchNormalization , GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ],
      "metadata": {
        "id": "VxOvczlTwR7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7El6nr8fuh3N"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}